name: EasyShop Application Deployment

on:
  workflow_dispatch:
  repository_dispatch:
    types: [deploy-applications]
  workflow_run:
    workflows: ["Terraform Infrastructure Provisioning"]
    types: [completed]
    branches: [main]

env:
  REGISTRY: ${{ secrets.TF_ACR_NAME }}.azurecr.io
  IMAGE_NAME: easyshop
  AKS_CLUSTER_NAME: ${{ secrets.TF_AKS_CLUSTER_NAME }}
  RESOURCE_GROUP: ${{ secrets.TF_PROJECT_NAME }}-rg
  NAMESPACE: easyshop

jobs:
  deploy-application:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'repository_dispatch' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') }}
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Azure Login
      uses: azure/login@v2
      with:
        creds: |
          {
            "clientId": "${{ secrets.ARM_CLIENT_ID }}",
            "clientSecret": "${{ secrets.ARM_CLIENT_SECRET }}",
            "subscriptionId": "${{ secrets.ARM_SUBSCRIPTION_ID }}",
            "tenantId": "${{ secrets.ARM_TENANT_ID }}"
          }

    - name: ğŸ¯ Generate SHA Version
      id: version
      run: |
        VERSION="${GITHUB_SHA::8}"
        echo "VERSION=$VERSION" >> $GITHUB_ENV
        echo "ğŸ“¦ Using SHA version: $VERSION"

    # âœ… FIXED: Set up Docker Buildx with container driver that supports cache
    - name: ğŸ‹ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: docker-container
        driver-opts: |
          image=moby/buildkit:latest
        use: true

    # âœ… FIXED: Use Azure CLI for ACR login (more reliable than docker-login action)
    - name: ğŸ”‘ Login to ACR
      run: |
        az acr login --name ${{ secrets.TF_ACR_NAME }}

    # âœ… FIXED: Build and Push with ACR registry cache (more reliable than GHA cache for ACR)
    # Modify your existing build step to capture the digest:
    # Modify your existing build step to capture the digest:
    - name: ğŸ—ï¸ Build and Push with Caching
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        platforms: linux/amd64
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        build-args: |
          GIT_SHA=${{ env.VERSION }}
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        outputs: type=image,name=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }},push=true

    # Then extract the digest from the build output:
    - name: ğŸ“Œ Get Image Digest from Build
      run: |
        # The build step outputs metadata including the digest
        IMAGE_DIGEST="${{ steps.build.outputs.digest }}"
        
        if [[ -n "$IMAGE_DIGEST" ]]; then
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${IMAGE_DIGEST}"
          echo "âœ… Got digest from build: $IMAGE_DIGEST"
        else
          # Fallback: extract from build metadata
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
          echo "âš ï¸ No digest from build, using tag: $IMAGE_WITH_DIGEST"
        fi
        
        echo "IMAGE_WITH_DIGEST=$IMAGE_WITH_DIGEST" >> $GITHUB_ENV
        echo "ğŸ“Œ Final image: $IMAGE_WITH_DIGEST"

    - name: âš™ï¸ Connect to AKS & Install Kustomize
      run: |
        echo "ğŸ”‘ Getting AKS credentials..."
        az aks get-credentials \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --overwrite-existing
        
        echo "ğŸ“¦ Installing kustomize..."
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/
#*************************************************************************
    # ADD THIS NEW DEBUG STEP HERE - BEFORE THE DEPLOYMENT STEP:
    - name: ğŸ” Debug SecretProviderClass Configuration
      run: |
        echo "ğŸ” Checking SecretProviderClass configuration..."
        
        # Show the actual SecretProviderClass that was applied
        echo "ğŸ“‹ SecretProviderClass YAML:"
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o yaml 2>/dev/null || echo "SecretProviderClass not found"
        
        # Check if CSI driver is installed
        echo ""
        echo "ğŸ” CSI Secret Store driver pods:"
        kubectl get pods -n kube-system | grep csi || echo "No CSI pods found"
        
        # Check events for CSI-related errors
        echo ""
        echo "ğŸ” Recent events in namespace:"
        kubectl get events -n easyshop --sort-by='.lastTimestamp' 2>/dev/null | tail -20 || echo "No events found"
        
        # Check if the managed identity exists and has correct permissions
        echo ""
        echo "ğŸ” Managed identity in SecretProviderClass:"
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o jsonpath='{.spec.parameters.userAssignedIdentityClientID}' 2>/dev/null || echo "Could not get managed identity ID"
        
        # Check AKS addons
        echo ""
        echo "ğŸ” AKS addons status:"
        az aks show --resource-group ${{ env.RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }} --query addonProfiles

#*************************************************************************

#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    - name: ğŸ”§ Fix CSI Secret Store Driver (Proper Installation)
      run: |
        echo "ğŸ”§ Installing CSI Secret Store driver components properly..."
        
        # Step 1: Check what's currently installed
        echo "ğŸ“‹ Current CSI drivers:"
        kubectl get csidrivers || echo "No CSI drivers found"
        
        echo ""
        echo "ğŸ“‹ Current CSI pods:"
        kubectl get pods -n kube-system | grep csi
        
        # Step 2: Install base CSI Secret Store driver (this creates the CSIDriver resource)
        echo ""
        echo "ğŸ“¦ Installing base CSI Secret Store driver..."
        kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/secrets-store-csi-driver/v1.4.0/deploy/secrets-store-csi-driver.yaml
        
        # Wait for base driver pods to be ready
        echo "â³ Waiting for base CSI driver pods..."
        kubectl wait --for=condition=ready pod -l app=secrets-store-csi-driver -n kube-system --timeout=180s
        
        # Step 3: Verify CSIDriver resource is now created
        echo ""
        echo "ğŸ” Checking if CSIDriver is now registered..."
        if kubectl get csidriver secrets-store.csi.x-k8s.io; then
          echo "âœ… Base CSI driver registered successfully!"
        else
          echo "âŒ Base CSI driver registration failed"
          echo "ğŸ“‹ Available CSI drivers:"
          kubectl get csidrivers
          exit 1
        fi
        
        # Step 4: Clean up any old Azure provider installations
        echo ""
        echo "ğŸ§¹ Cleaning up old Azure provider installations..."
        kubectl delete daemonset csi-secrets-store-provider-azure -n kube-system --ignore-not-found=true
        kubectl delete serviceaccount csi-secrets-store-provider-azure -n kube-system --ignore-not-found=true
        
        # Step 5: Install Azure provider (this works with the base driver)
        echo "ğŸ“¦ Installing Azure CSI provider..."
        kubectl apply -f https://raw.githubusercontent.com/Azure/secrets-store-csi-driver-provider-azure/v1.5.0/deployment/provider-azure-installer.yaml
        
        # Wait for Azure provider to be ready
        echo "â³ Waiting for Azure provider pods..."
        kubectl wait --for=condition=ready pod -l app=csi-secrets-store-provider-azure -n kube-system --timeout=120s
        
        # Step 6: Restart the AKS-managed CSI driver to pick up changes
        echo ""
        echo "ğŸ”„ Restarting AKS CSI driver to pick up changes..."
        kubectl rollout restart daemonset/aks-secrets-store-csi-driver -n kube-system
        kubectl rollout status daemonset/aks-secrets-store-csi-driver -n kube-system --timeout=120s
        
        # Step 7: Final verification
        echo ""
        echo "ğŸ” Final verification..."
        echo "CSI drivers:"
        kubectl get csidrivers
        
        echo ""
        echo "CSI pods:"
        kubectl get pods -n kube-system | grep csi
        
        # Step 8: Test with a simple SecretProviderClass
        echo ""
        echo "ğŸ§ª Testing CSI driver with simple SecretProviderClass..."
        cat << EOF | kubectl apply -f -
        apiVersion: secrets-store.csi.x-k8s.io/v1
        kind: SecretProviderClass
        metadata:
          name: test-csi-driver
          namespace: default
        spec:
          provider: azure
          parameters:
            keyvaultName: "test"
            tenantId: "test"
            objects: |
              array:
                - |
                  objectName: test-secret
                  objectType: secret
        EOF
        
        if kubectl get secretproviderclass test-csi-driver -n default; then
          echo "âœ… CSI driver is working - SecretProviderClass created successfully!"
          kubectl delete secretproviderclass test-csi-driver -n default
        else
          echo "âŒ CSI driver still not working properly"
          exit 1
        fi
        
        # Step 9: Clean up stuck pods that were failing before
        echo ""
        echo "ğŸ§¹ Cleaning up previously stuck pods..."
        kubectl delete pods -n easyshop --field-selector=status.phase=Pending --force --grace-period=0 || true
        kubectl delete pods -n easyshop --field-selector=status.phase=ContainerCreating --force --grace-period=0 || true
        
        echo ""
        echo "âœ… CSI Secret Store driver is now properly installed and configured!"
        echo "ğŸ”„ Previously stuck pods have been cleaned up and will be recreated"
#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#====================================================================
#====================================================================
    # REPLACE your deployment step with this bypass version:
    - name: ğŸš€ Blue-Green Deployment - Key Vault Bypass
      run: |
        set -e
        
        echo "ğŸš€ Starting Blue-Green Deployment (Key Vault bypass)..."
        
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        cd kubernetes
        
        # Create a temporary kustomization without Key Vault dependencies
        echo "ğŸ“ Creating temporary kustomization without Key Vault..."
        cat > kustomization-bypass.yaml << 'EOF'
        apiVersion: kustomize.config.k8s.io/v1beta1
        kind: Kustomization
        
        namespace: easyshop
        
        resources:
        - 01-namespace.yaml
        - 04-configmap.yaml
        - 05-secrets.yaml
        - 08a-blue-deployment.yml
        - 08b-green-deployment.yaml
        - 09-easyshop-service.yaml
        - 10-ingress.yaml
        - 11-hpa.yaml
        - cert-manager-issuer.yaml
        - easyshop-certificate.yaml
        
        commonLabels:
          app.kubernetes.io/name: easyshop
          app.kubernetes.io/part-of: easyshop-platform
          app.kubernetes.io/managed-by: kustomize
          environment: production
        
        images:
        - name: easyshop
          newTag: latest
        
        replicas:
        - name: easyshop-blue
          count: 2
        - name: easyshop-green
          count: 2
        
        # Remove Key Vault volume mounts from deployments
        patches:
        - patch: |-
            - op: remove
              path: /spec/template/spec/volumes
            - op: remove  
              path: /spec/template/spec/containers/0/volumeMounts
          target:
            kind: Deployment
            name: easyshop-blue
        - patch: |-
            - op: remove
              path: /spec/template/spec/volumes
            - op: remove
              path: /spec/template/spec/containers/0/volumeMounts
          target:
            kind: Deployment
            name: easyshop-green
        # Remove Key Vault environment variables and use simple values
        - patch: |-
            - op: replace
              path: /spec/template/spec/containers/0/env
              value:
                - name: NODE_ENV
                  value: "production"
                - name: NEXTAUTH_URL
                  valueFrom:
                    configMapKeyRef:
                      name: easyshop-config
                      key: NEXTAUTH_URL
                - name: NEXT_PUBLIC_API_URL
                  valueFrom:
                    configMapKeyRef:
                      name: easyshop-config
                      key: NEXT_PUBLIC_API_URL
                - name: NEXTAUTH_SECRET
                  valueFrom:
                    secretKeyRef:
                      name: easyshop-secrets
                      key: NEXTAUTH_SECRET
                - name: JWT_SECRET
                  valueFrom:
                    secretKeyRef:
                      name: easyshop-secrets
                      key: JWT_SECRET
                - name: MONGODB_URI
                  value: "mongodb://host.docker.internal:27017/easyshop"
                - name: REDIS_URI
                  value: "redis://host.docker.internal:6379"
          target:
            kind: Deployment
            name: easyshop-blue
        - patch: |-
            - op: replace
              path: /spec/template/spec/containers/0/env
              value:
                - name: NODE_ENV
                  value: "production"
                - name: NEXTAUTH_URL
                  valueFrom:
                    configMapKeyRef:
                      name: easyshop-config
                      key: NEXTAUTH_URL
                - name: NEXT_PUBLIC_API_URL
                  valueFrom:
                    configMapKeyRef:
                      name: easyshop-config
                      key: NEXT_PUBLIC_API_URL
                - name: NEXTAUTH_SECRET
                  valueFrom:
                    secretKeyRef:
                      name: easyshop-secrets
                      key: NEXTAUTH_SECRET
                - name: JWT_SECRET
                  valueFrom:
                    secretKeyRef:
                      name: easyshop-secrets
                      key: JWT_SECRET
                - name: MONGODB_URI
                  value: "mongodb://host.docker.internal:27017/easyshop"
                - name: REDIS_URI
                  value: "redis://host.docker.internal:6379"
          target:
            kind: Deployment
            name: easyshop-green
        EOF
        
        # Update image in bypass kustomization
        echo "ğŸ“ Updating image to: ${{ env.IMAGE_WITH_DIGEST }}"
        kustomize edit set image easyshop=${{ env.IMAGE_WITH_DIGEST }} --file kustomization-bypass.yaml
        
        # Test build with bypass
        echo "ğŸ” Testing bypass kustomization build..."
        if kustomize build -f kustomization-bypass.yaml > /tmp/manifests.yaml; then
          RESOURCE_COUNT=$(grep -c "^kind:" /tmp/manifests.yaml)
          echo "âœ… Generated $RESOURCE_COUNT resources (bypass mode)"
          
          echo "ğŸ” Resource types:"
          grep "^kind:" /tmp/manifests.yaml | sort | uniq -c
        else
          echo "âŒ Bypass kustomization failed too!"
          exit 1
        fi
        
        # Apply bypass manifests
        echo "ğŸ“¦ Applying bypass manifests..."
        kubectl apply -f /tmp/manifests.yaml
        
        # Determine target deployment
        CURRENT_DEPLOYMENT=$(kubectl get svc easyshop-service -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.selector.app}' 2>/dev/null || echo "easyshop-blue")
        
        if [ "$CURRENT_DEPLOYMENT" = "easyshop-blue" ]; then
          TARGET_DEPLOYMENT="easyshop-green"
          echo "ğŸ”„ Current: BLUE â†’ Deploying to: GREEN"
        else
          TARGET_DEPLOYMENT="easyshop-blue"
          echo "ğŸ”„ Current: GREEN â†’ Deploying to: BLUE"
        fi
        
        # Wait for deployment
        echo "â³ Waiting for $TARGET_DEPLOYMENT (bypass mode)..."
        if kubectl rollout status deployment/$TARGET_DEPLOYMENT -n ${{ env.NAMESPACE }} --timeout=300s; then
          echo "âœ… Deployment successful!"
          
          # Switch traffic
          kubectl patch svc easyshop-service -n ${{ env.NAMESPACE }} \
            -p '{"spec":{"selector":{"app":"'$TARGET_DEPLOYMENT'"}}}'
          
          # Scale down old
          if [ "$CURRENT_DEPLOYMENT" != "$TARGET_DEPLOYMENT" ]; then
            kubectl scale deployment/$CURRENT_DEPLOYMENT --replicas=0 -n ${{ env.NAMESPACE }} || true
          fi
          
          echo "ğŸ‰ Blue-Green deployment completed (bypass mode)!"
          echo "âš ï¸  Note: Running without Key Vault integration"
          
        else
          echo "âŒ Deployment failed even in bypass mode"
          kubectl get pods -n ${{ env.NAMESPACE }}
          kubectl describe pods -n ${{ env.NAMESPACE }} -l app=$TARGET_DEPLOYMENT
          exit 1
        fi
#====================================================================
#====================================================================

    - name: ğŸ” Verify Deployment
      run: |
        echo "ğŸ” Verifying deployment..."
        
        # Check pods
        echo "ğŸ“¦ Pods in ${{ env.NAMESPACE }}:"
        kubectl get pods -n ${{ env.NAMESPACE }}
        
        # Check services
        echo "ğŸŒ Services in ${{ env.NAMESPACE }}:"
        kubectl get svc -n ${{ env.NAMESPACE }}
        
        # Get the actual hostname from ingress
        echo "ğŸ”— Ingress in ${{ env.NAMESPACE }}:"
        kubectl get ingress -n ${{ env.NAMESPACE }}
        
        INGRESS_HOST=$(kubectl get ingress easyshop-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "${{ secrets.TF_DNS_ZONE_NAME }}")
        echo "INGRESS_HOST=$INGRESS_HOST" >> $GITHUB_ENV

    - name: ğŸ“‹ Deployment Summary
      run: |
        echo "ğŸ‰ Blue-Green Deployment Complete!"
        echo "=================================="
        echo "ğŸ·ï¸  SHA Version: ${{ env.VERSION }}"
        echo "ğŸ‹ Image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
        echo "ğŸ“Œ Digest: ${{ env.IMAGE_WITH_DIGEST }}"
        echo "ğŸ¯ Cluster: ${{ env.AKS_CLUSTER_NAME }}"
        echo "ğŸ  Namespace: ${{ env.NAMESPACE }}"
        echo "ğŸŒ URL: https://${{ env.INGRESS_HOST }}"
        echo "ğŸ“ Commit: ${{ github.sha }}"
        echo ""
        echo "ğŸ”„ Blue-Green Strategy:"
        echo "  âœ… Deployed to inactive slot with digest pinning"
        echo "  âœ… Verified health and readiness probes"
        echo "  âœ… Switched traffic with zero downtime"
        echo "  âœ… Scaled down old deployment"
        echo "=================================="