name: EasyShop Application Deployment

on:
  workflow_dispatch:
  repository_dispatch:
    types: [deploy-applications]
  workflow_run:
    workflows: ["Terraform Infrastructure Provisioning"]
    types: [completed]
    branches: [main]

env:
  REGISTRY: ${{ secrets.TF_ACR_NAME }}.azurecr.io
  IMAGE_NAME: easyshop
  AKS_CLUSTER_NAME: ${{ secrets.TF_AKS_CLUSTER_NAME }}
  RESOURCE_GROUP: ${{ secrets.TF_PROJECT_NAME }}-rg
  NAMESPACE: easyshop

jobs:
  deploy-application:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'repository_dispatch' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') }}
    
    steps:
    - name: 🚀 Checkout Repository
      uses: actions/checkout@v4

    - name: 🔐 Azure Login
      uses: azure/login@v2
      with:
        creds: |
          {
            "clientId": "${{ secrets.ARM_CLIENT_ID }}",
            "clientSecret": "${{ secrets.ARM_CLIENT_SECRET }}",
            "subscriptionId": "${{ secrets.ARM_SUBSCRIPTION_ID }}",
            "tenantId": "${{ secrets.ARM_TENANT_ID }}"
          }

    - name: 🎯 Generate SHA Version
      id: version
      run: |
        VERSION="${GITHUB_SHA::8}"
        echo "VERSION=$VERSION" >> $GITHUB_ENV
        echo "📦 Using SHA version: $VERSION"

    # ✅ FIXED: Set up Docker Buildx with container driver that supports cache
    - name: 🐋 Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: docker-container
        driver-opts: |
          image=moby/buildkit:latest
        use: true

    # ✅ FIXED: Use Azure CLI for ACR login (more reliable than docker-login action)
    - name: 🔑 Login to ACR
      run: |
        az acr login --name ${{ secrets.TF_ACR_NAME }}

    # ✅ FIXED: Build and Push with ACR registry cache (more reliable than GHA cache for ACR)
    # Modify your existing build step to capture the digest:
    # Modify your existing build step to capture the digest:
    - name: 🏗️ Build and Push with Caching
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        platforms: linux/amd64
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        build-args: |
          GIT_SHA=${{ env.VERSION }}
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        outputs: type=image,name=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }},push=true

    # Then extract the digest from the build output:
    - name: 📌 Get Image Digest from Build
      run: |
        # The build step outputs metadata including the digest
        IMAGE_DIGEST="${{ steps.build.outputs.digest }}"
        
        if [[ -n "$IMAGE_DIGEST" ]]; then
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${IMAGE_DIGEST}"
          echo "✅ Got digest from build: $IMAGE_DIGEST"
        else
          # Fallback: extract from build metadata
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
          echo "⚠️ No digest from build, using tag: $IMAGE_WITH_DIGEST"
        fi
        
        echo "IMAGE_WITH_DIGEST=$IMAGE_WITH_DIGEST" >> $GITHUB_ENV
        echo "📌 Final image: $IMAGE_WITH_DIGEST"

    - name: ⚙️ Connect to AKS & Install Kustomize
      run: |
        echo "🔑 Getting AKS credentials..."
        az aks get-credentials \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --overwrite-existing
        
        echo "📦 Installing kustomize..."
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/
#===========================================================================
    # Workflow-compatible CRD creation
    - name: 🔧 Install CSI Driver CRDs
      run: |
        echo "📦 Installing CSI Driver CRDs..."

        # Apply SecretProviderClasses CRD
        kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/secrets-store-csi-driver/master/deploy/secrets-store.csi.x-k8s.io_secretproviderclasses.yaml

        # Apply SecretProviderClassPodStatuses CRD
        kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/secrets-store-csi-driver/master/deploy/secrets-store.csi.x-k8s.io_secretproviderclasspodstatuses.yaml

        echo "⏳ Giving CRDs some time to register..."
        sleep 20

        # Verify they exist
        kubectl get crd secretproviderclasses.secrets-store.csi.x-k8s.io && kubectl get crd secretproviderclasspodstatuses.secrets-store.csi.x-k8s.io

        echo "✅ CRDs are installed!"


#===========================================================================

#*************************************************************************
    # ADD THIS NEW DEBUG STEP HERE - BEFORE THE DEPLOYMENT STEP:
    - name: 🔍 Debug SecretProviderClass Configuration
      run: |
        echo "🔍 Checking SecretProviderClass configuration..."
        
        # Show the actual SecretProviderClass that was applied
        echo "📋 SecretProviderClass YAML:"
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o yaml 2>/dev/null || echo "SecretProviderClass not found"
        
        # Check if CSI driver is installed
        echo ""
        echo "🔍 CSI Secret Store driver pods:"
        kubectl get pods -n kube-system | grep csi || echo "No CSI pods found"
        
        # Check events for CSI-related errors
        echo ""
        echo "🔍 Recent events in namespace:"
        kubectl get events -n easyshop --sort-by='.lastTimestamp' 2>/dev/null | tail -20 || echo "No events found"
        
        # Check if the managed identity exists and has correct permissions
        echo ""
        echo "🔍 Managed identity in SecretProviderClass:"
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o jsonpath='{.spec.parameters.userAssignedIdentityClientID}' 2>/dev/null || echo "Could not get managed identity ID"
        
        # Check AKS addons
        echo ""
        echo "🔍 AKS addons status:"
        az aks show --resource-group ${{ env.RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }} --query addonProfiles

#*************************************************************************

#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    - name: 🔧 Fix CSI Driver Name (Selective Fix)
      run: |
        echo "🔧 Fixing CSI driver name references (selective)..."
        cd kubernetes
      
        # Show current state
        echo "📋 Current CSI driver references:"
        grep -rn "secrets-store.csi" . || echo "No references found"
        
        echo ""
        echo "🔧 Applying selective fixes..."
        
        # Fix ONLY the driver name in volume mounts, NOT the apiVersion in SecretProviderClass
        # Update driver in deployment volume mounts
        sed -i 's/driver: secrets-store\.csi\.x-k8s\.io/driver: secrets-store.csi.k8s.io/g' *.yaml *.yml || true
        
        # IMPORTANT: Revert the apiVersion in SecretProviderClass back to x-k8s.io
        if [[ -f "15-keyvault-secret-provider.yaml" ]]; then
          echo "🔄 Reverting SecretProviderClass apiVersion to match installed CRDs..."
          sed -i 's/apiVersion: secrets-store\.csi\.k8s\.io\/v1/apiVersion: secrets-store.csi.x-k8s.io\/v1/g' 15-keyvault-secret-provider.yaml
        fi
        
        echo ""
        echo "✅ After selective fix:"
        echo "📋 SecretProviderClass apiVersion (should be x-k8s.io):"
        grep "apiVersion:" 15-keyvault-secret-provider.yaml || echo "File not found"
        
        echo "📋 Volume driver names (should be k8s.io):"
        grep -n "driver: secrets-store" *.yaml *.yml || echo "No driver references found"
        
        # Rebuild manifests with selective fixes
        echo ""
        echo "🔍 Rebuilding manifests with selective fixes..."
        kustomize edit set image easyshop=${{ env.IMAGE_WITH_DIGEST }}
        kustomize build . > /tmp/manifests-selective.yaml
        
        # Verify the API versions in manifests
        echo "📋 Checking API versions in generated manifests:"
        grep -A 2 -B 2 "kind: SecretProviderClass" /tmp/manifests-selective.yaml || echo "No SecretProviderClass found"
        
        # Apply the selectively fixed manifests
        echo ""
        echo "📦 Applying selectively fixed manifests..."
        kubectl apply -f /tmp/manifests-selective.yaml
        
        echo "✅ Selective CSI driver fix completed!"
#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

#====================================================================
#====================================================================
    - name: 🚀 Blue-Green Deployment - Identity Fix
      run: |
        set -e
        
        echo "🚀 Starting Blue-Green Deployment with Identity Fix..."
        
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        cd kubernetes
        
        # Update image
        echo "📝 Updating image to: ${{ env.IMAGE_WITH_DIGEST }}"
        kustomize edit set image easyshop=${{ env.IMAGE_WITH_DIGEST }}
        
        # Build manifests
        echo "🔍 Building manifests..."
        kustomize build . > /tmp/manifests.yaml
        
        RESOURCE_COUNT=$(grep -c "^kind:" /tmp/manifests.yaml)
        echo "✅ Generated $RESOURCE_COUNT resources"
        
        # Apply all manifests except SecretProviderClass first
        echo "📦 Applying base resources..."
        kubectl apply -f /tmp/manifests.yaml
        
        # CRITICAL FIX: Get the correct AKS addon identity for Key Vault CSI
        echo "🔍 Getting AKS addon identity for Key Vault CSI..."
        AKS_ADDON_IDENTITY_CLIENT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.clientId \
          -o tsv 2>/dev/null || echo "")
        
        if [[ -z "$AKS_ADDON_IDENTITY_CLIENT_ID" || "$AKS_ADDON_IDENTITY_CLIENT_ID" == "null" ]]; then
          echo "❌ Key Vault addon identity not found. Checking if addon is enabled..."
          
          # Check if addon is enabled
          ADDON_ENABLED=$(az aks show \
            --resource-group ${{ env.RESOURCE_GROUP }} \
            --name ${{ env.AKS_CLUSTER_NAME }} \
            --query addonProfiles.azureKeyvaultSecretsProvider.enabled \
            -o tsv 2>/dev/null || echo "false")
          
          if [[ "$ADDON_ENABLED" != "true" ]]; then
            echo "🔧 Enabling Key Vault addon..."
            az aks addon enable \
              --resource-group ${{ env.RESOURCE_GROUP }} \
              --name ${{ env.AKS_CLUSTER_NAME }} \
              --addon azure-keyvault-secrets-provider
            
            # Wait for addon to be ready
            echo "⏳ Waiting for addon to initialize..."
            sleep 30
            
            # Get the identity again
            AKS_ADDON_IDENTITY_CLIENT_ID=$(az aks show \
              --resource-group ${{ env.RESOURCE_GROUP }} \
              --name ${{ env.AKS_CLUSTER_NAME }} \
              --query addonProfiles.azureKeyvaultSecretsProvider.identity.clientId \
              -o tsv)
          else
            echo "❌ Addon is enabled but identity not found. This might be a timing issue."
            exit 1
          fi
        fi
        
        echo "✅ Found AKS addon identity: $AKS_ADDON_IDENTITY_CLIENT_ID"
        
        # Get Key Vault name and tenant ID
        KEY_VAULT_NAME=$(az keyvault list \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --query "[?starts_with(name, '${{ secrets.TF_PROJECT_NAME }}-kv')].name" \
          -o tsv | head -1)
        
        TENANT_ID=$(az account show --query tenantId -o tsv)
        
        echo "🔑 Key Vault: $KEY_VAULT_NAME"
        echo "🏢 Tenant ID: $TENANT_ID"
        
        # CRITICAL: Grant the AKS addon identity access to Key Vault
        echo "🔐 Granting AKS addon identity access to Key Vault..."
        
        # Get the addon identity object ID
        ADDON_IDENTITY_OBJECT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.objectId \
          -o tsv)
        
        # Set Key Vault access policy for the addon identity
        az keyvault set-policy \
          --name $KEY_VAULT_NAME \
          --object-id $ADDON_IDENTITY_OBJECT_ID \
          --secret-permissions get list \
          --output none || echo "⚠️ Access policy may already exist"
        
        echo "✅ Key Vault access configured for addon identity"
        
        # Update SecretProviderClass with correct identity
        echo "🔧 Updating SecretProviderClass with AKS addon identity..."
        kubectl patch secretproviderclass easyshop-keyvault-secrets -n easyshop --type='merge' -p="{
          \"spec\": {
            \"parameters\": {
              \"userAssignedIdentityClientID\": \"$AKS_ADDON_IDENTITY_CLIENT_ID\",
              \"keyvaultName\": \"$KEY_VAULT_NAME\",
              \"tenantId\": \"$TENANT_ID\"
            }
          }
        }"
        
        echo "✅ SecretProviderClass updated with correct AKS addon identity"
        
        # Clean up any stuck pods to force recreation with new identity
        echo "🧹 Cleaning up stuck pods to apply identity fix..."
        kubectl delete pods -n easyshop --field-selector=status.phase=Pending --force --grace-period=0 || true
        kubectl delete pods -n easyshop --field-selector=status.phase=ContainerCreating --force --grace-period=0 || true
        kubectl delete pods -n easyshop -l app=easyshop-blue --force --grace-period=0 || true
        kubectl delete pods -n easyshop -l app=easyshop-green --force --grace-period=0 || true
        
        # Wait a moment for cleanup
        echo "⏳ Waiting for cleanup to complete..."
        sleep 15
        
        # Determine target deployment
        CURRENT_DEPLOYMENT=$(kubectl get svc easyshop-service -n easyshop -o jsonpath='{.spec.selector.app}' 2>/dev/null || echo "easyshop-blue")
        
        if [ "$CURRENT_DEPLOYMENT" = "easyshop-blue" ]; then
          TARGET_DEPLOYMENT="easyshop-green"
          echo "🔄 Current: BLUE → Target: GREEN"
        else
          TARGET_DEPLOYMENT="easyshop-blue"
          echo "🔄 Current: GREEN → Target: BLUE"
        fi
        
        # Wait for deployment rollout with extended timeout for identity propagation
        echo "⏳ Waiting for $TARGET_DEPLOYMENT rollout (extended timeout for identity fix)..."
        if kubectl rollout status deployment/$TARGET_DEPLOYMENT -n easyshop --timeout=600s; then
          echo "✅ Rollout successful with identity fix!"
          
          # Verify pods can mount secrets
          echo "🔍 Verifying secret mounting..."
          kubectl get pods -n easyshop -l app=$TARGET_DEPLOYMENT
          
          # Check for any CSI-related events
          echo "🔍 Checking for CSI driver events..."
          kubectl get events -n easyshop --field-selector reason=FailedMount || echo "No mount failures found"
          
          # Switch traffic
          echo "🔀 Switching traffic to $TARGET_DEPLOYMENT..."
          kubectl patch svc easyshop-service -n easyshop \
            -p '{"spec":{"selector":{"app":"'$TARGET_DEPLOYMENT'"}}}'
          
          # Scale down old deployment
          if [ "$CURRENT_DEPLOYMENT" != "$TARGET_DEPLOYMENT" ]; then
            echo "📉 Scaling down $CURRENT_DEPLOYMENT..."
            kubectl scale deployment/$CURRENT_DEPLOYMENT --replicas=0 -n easyshop || true
          fi
          
          echo ""
          echo "🎉 BLUE-GREEN DEPLOYMENT SUCCESSFUL WITH IDENTITY FIX!"
          echo "=================================================="
          echo "🎯 Active: $TARGET_DEPLOYMENT"
          echo "🐋 Image: ${{ env.IMAGE_WITH_DIGEST }}"
          echo "🔐 Identity: $AKS_ADDON_IDENTITY_CLIENT_ID"
          echo "🗝️ Key Vault: $KEY_VAULT_NAME"
          
        else
          echo "❌ Deployment rollout failed"
          echo "📋 Pod status:"
          kubectl get pods -n easyshop -l app=$TARGET_DEPLOYMENT
          echo ""
          echo "📋 Pod describe (last 20 lines):"
          kubectl describe pods -n easyshop -l app=$TARGET_DEPLOYMENT | tail -20
          echo ""
          echo "📋 CSI driver events:"
          kubectl get events -n easyshop --field-selector reason=FailedMount,type=Warning || echo "No CSI events"
          exit 1
        fi
#====================================================================
    - name: 🔧 Fix SecretProviderClass tenantId
      run: |
        echo "🔧 Fixing missing tenantId in SecretProviderClass..."
        
        # Get tenant ID from Azure CLI
        TENANT_ID=$(az account show --query tenantId -o tsv)
        echo "🔍 Tenant ID: $TENANT_ID"
        
        # Get current SecretProviderClass and check if tenantId is missing
        echo "📋 Checking current SecretProviderClass..."
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o jsonpath='{.spec.parameters.tenantId}' || echo "tenantId missing"
        
        # Patch the SecretProviderClass to add tenantId
        echo "🔧 Adding tenantId to SecretProviderClass..."
        kubectl patch secretproviderclass easyshop-keyvault-secrets -n easyshop --type='merge' -p="{\"spec\":{\"parameters\":{\"tenantId\":\"$TENANT_ID\"}}}"
        
        # Verify the fix
        echo "✅ Verification - tenantId now set to:"
        kubectl get secretproviderclass easyshop-keyvault-secrets -n easyshop -o jsonpath='{.spec.parameters.tenantId}'
        echo ""
        
        # Clean up stuck pods
        echo "🧹 Cleaning up stuck pods to retry with correct config..."
        kubectl delete pods -n easyshop --field-selector=status.phase=ContainerCreating --force --grace-period=0 || true
        
        echo "✅ SecretProviderClass tenantId fix completed!"

#====================================================================

    - name: 🔍 Verify Deployment
      run: |
        echo "🔍 Verifying deployment..."
        
        # Check pods
        echo "📦 Pods in ${{ env.NAMESPACE }}:"
        kubectl get pods -n ${{ env.NAMESPACE }}
        
        # Check services
        echo "🌐 Services in ${{ env.NAMESPACE }}:"
        kubectl get svc -n ${{ env.NAMESPACE }}
        
        # Get the actual hostname from ingress
        echo "🔗 Ingress in ${{ env.NAMESPACE }}:"
        kubectl get ingress -n ${{ env.NAMESPACE }}
        
        INGRESS_HOST=$(kubectl get ingress easyshop-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "${{ secrets.TF_DNS_ZONE_NAME }}")
        echo "INGRESS_HOST=$INGRESS_HOST" >> $GITHUB_ENV

    - name: 📋 Deployment Summary
      run: |
        echo "🎉 Blue-Green Deployment Complete!"
        echo "=================================="
        echo "🏷️  SHA Version: ${{ env.VERSION }}"
        echo "🐋 Image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
        echo "📌 Digest: ${{ env.IMAGE_WITH_DIGEST }}"
        echo "🎯 Cluster: ${{ env.AKS_CLUSTER_NAME }}"
        echo "🏠 Namespace: ${{ env.NAMESPACE }}"
        echo "🌐 URL: https://${{ env.INGRESS_HOST }}"
        echo "📍 Commit: ${{ github.sha }}"
        echo ""
        echo "🔄 Blue-Green Strategy:"
        echo "  ✅ Deployed to inactive slot with digest pinning"
        echo "  ✅ Verified health and readiness probes"
        echo "  ✅ Switched traffic with zero downtime"
        echo "  ✅ Scaled down old deployment"
        echo "=================================="