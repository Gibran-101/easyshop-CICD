name: EasyShop Application Deployment

on:
  workflow_dispatch:
  repository_dispatch:
    types: [deploy-applications]
  workflow_run:
    workflows: ["Terraform Infrastructure Provisioning"]
    types: [completed]
    branches: [main]

env:
  REGISTRY: ${{ secrets.TF_ACR_NAME }}.azurecr.io
  IMAGE_NAME: easyshop
  AKS_CLUSTER_NAME: ${{ secrets.TF_AKS_CLUSTER_NAME }}
  RESOURCE_GROUP: ${{ secrets.TF_PROJECT_NAME }}-rg
  NAMESPACE: easyshop

jobs:
  deploy-application:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'repository_dispatch' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') }}
    
    steps:
    - name: üöÄ Checkout Repository
      uses: actions/checkout@v4

    - name: üîê Azure Login
      uses: azure/login@v2
      with:
        creds: |
          {
            "clientId": "${{ secrets.ARM_CLIENT_ID }}",
            "clientSecret": "${{ secrets.ARM_CLIENT_SECRET }}",
            "subscriptionId": "${{ secrets.ARM_SUBSCRIPTION_ID }}",
            "tenantId": "${{ secrets.ARM_TENANT_ID }}"
          }

    - name: üéØ Generate SHA Version
      id: version
      run: |
        VERSION="${GITHUB_SHA::8}"
        echo "VERSION=$VERSION" >> $GITHUB_ENV
        echo "üì¶ Using SHA version: $VERSION"

    - name: üêã Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: docker-container
        driver-opts: |
          image=moby/buildkit:latest
        use: true

    - name: üîë Login to ACR
      run: |
        az acr login --name ${{ secrets.TF_ACR_NAME }}

    - name: üèóÔ∏è Build and Push with Caching
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        platforms: linux/amd64
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        build-args: |
          GIT_SHA=${{ env.VERSION }}
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        outputs: type=image,name=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }},push=true

    - name: üìå Get Image Digest from Build
      run: |
        IMAGE_DIGEST="${{ steps.build.outputs.digest }}"
        
        if [[ -n "$IMAGE_DIGEST" ]]; then
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${IMAGE_DIGEST}"
          echo "‚úÖ Got digest from build: $IMAGE_DIGEST"
        else
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
          echo "‚ö†Ô∏è No digest from build, using tag: $IMAGE_WITH_DIGEST"
        fi
        
        echo "IMAGE_WITH_DIGEST=$IMAGE_WITH_DIGEST" >> $GITHUB_ENV
        echo "üìå Final image: $IMAGE_WITH_DIGEST"

    - name: ‚öôÔ∏è Connect to AKS & Install Kustomize
      run: |
        echo "üîë Getting AKS credentials..."
        az aks get-credentials \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --overwrite-existing
        
        echo "üì¶ Installing kustomize..."
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/
    # CRITICAL FIX: Enable Key Vault addon and configure identity properly
    - name: üîß Setup Key Vault CSI Driver and Identity
      run: |
        echo "üîß Setting up Key Vault CSI Driver and Identity..."
        
        # Ensure Key Vault addon is enabled
        echo "üì¶ Enabling Key Vault CSI addon..."
        az aks addon enable \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --addon azure-keyvault-secrets-provider \
          --output none || echo "Addon already enabled"
        
        # Wait for addon to be ready
        echo "‚è≥ Waiting for addon to initialize..."
        sleep 30
        
        # Get the AKS addon identity (this is the correct identity to use)
        echo "üîç Getting AKS addon identity..."
        AKS_ADDON_IDENTITY_CLIENT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.clientId \
          -o tsv)
        
        AKS_ADDON_IDENTITY_OBJECT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.objectId \
          -o tsv)
        
        TENANT_ID=$(az account show --query tenantId -o tsv)
        
        echo "‚úÖ AKS Addon Identity Client ID: $AKS_ADDON_IDENTITY_CLIENT_ID"
        echo "‚úÖ AKS Addon Identity Object ID: $AKS_ADDON_IDENTITY_OBJECT_ID"
        echo "‚úÖ Tenant ID: $TENANT_ID"
        
        # Store these for later use
        echo "AKS_ADDON_IDENTITY_CLIENT_ID=$AKS_ADDON_IDENTITY_CLIENT_ID" >> $GITHUB_ENV
        echo "AKS_ADDON_IDENTITY_OBJECT_ID=$AKS_ADDON_IDENTITY_OBJECT_ID" >> $GITHUB_ENV
        echo "TENANT_ID=$TENANT_ID" >> $GITHUB_ENV
        
        # Get Key Vault name
        KEY_VAULT_NAME=$(az keyvault list \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --query "[?starts_with(name, '${{ secrets.TF_PROJECT_NAME }}-kv')].name" \
          -o tsv | head -1)
        
        echo "üîë Key Vault: $KEY_VAULT_NAME"
        echo "KEY_VAULT_NAME=$KEY_VAULT_NAME" >> $GITHUB_ENV
        
        # Grant Key Vault access to the AKS addon identity
        echo "üîê Granting Key Vault access to AKS addon identity..."
        az keyvault set-policy \
          --name $KEY_VAULT_NAME \
          --object-id $AKS_ADDON_IDENTITY_OBJECT_ID \
          --secret-permissions get list \
          --output none
        
        echo "‚úÖ Key Vault access configured!"
        
        # Verify CSI driver pods are running
        echo "üîç Checking CSI driver pods..."
        kubectl get pods -n kube-system -l app=secrets-store-csi-driver || echo "CSI driver pods not found"
        kubectl get pods -n kube-system -l app=secrets-store-provider-azure || echo "Azure provider pods not found"

    # STEP 1: Configure SecretProviderClass FIRST
    - name: üîß Configure SecretProviderClass BEFORE Deployment
      run: |
        echo "üîß Configuring SecretProviderClass with actual values..."
        
        # Get all required values
        TENANT_ID=$(az account show --query tenantId -o tsv)
        KEY_VAULT_NAME=$(az keyvault list \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --query "[?starts_with(name, '${{ secrets.TF_PROJECT_NAME }}-kv')].name" \
          -o tsv | head -1)
        
        # Get addon identity (fixed the query path)
        AKS_ADDON_IDENTITY_CLIENT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.clientId \
          -o tsv)
        
        # FIXED: Get object ID with correct query path
        ADDON_IDENTITY_OBJECT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query addonProfiles.azureKeyvaultSecretsProvider.identity.objectId \
          -o tsv)
        
        echo "‚úÖ Values retrieved:"
        echo "  TENANT_ID: $TENANT_ID"
        echo "  KEY_VAULT_NAME: $KEY_VAULT_NAME"
        echo "  AKS_ADDON_IDENTITY_CLIENT_ID: $AKS_ADDON_IDENTITY_CLIENT_ID"
        echo "  ADDON_IDENTITY_OBJECT_ID: $ADDON_IDENTITY_OBJECT_ID"
        
        # Validate ALL values are present
        if [[ -z "$TENANT_ID" || -z "$KEY_VAULT_NAME" || -z "$AKS_ADDON_IDENTITY_CLIENT_ID" || -z "$ADDON_IDENTITY_OBJECT_ID" ]]; then
          echo "‚ùå Missing required values!"
          echo "  TENANT_ID: ${TENANT_ID:-MISSING}"
          echo "  KEY_VAULT_NAME: ${KEY_VAULT_NAME:-MISSING}"
          echo "  AKS_ADDON_IDENTITY_CLIENT_ID: ${AKS_ADDON_IDENTITY_CLIENT_ID:-MISSING}"
          echo "  ADDON_IDENTITY_OBJECT_ID: ${ADDON_IDENTITY_OBJECT_ID:-MISSING}"
          exit 1
        fi
        
        # Set Key Vault access policy FIRST
        echo "üîê Setting Key Vault access policy..."
        az keyvault set-policy \
          --name "$KEY_VAULT_NAME" \
          --object-id "$ADDON_IDENTITY_OBJECT_ID" \
          --secret-permissions get list \
          --output table
        
        echo "‚úÖ Key Vault access policy configured!"
        
        # Update the SecretProviderClass file BEFORE building manifests
        cd kubernetes
        
        # Show current placeholders
        echo "üìã Current placeholders in file:"
        grep -E "PLACEHOLDER|keyvaultName|tenantId|userAssignedIdentityClientID" 15-keyvault-secret-provider.yaml
        
        # Replace placeholders with actual values
        sed -i "s/PLACEHOLDER_KEYVAULT_NAME/$KEY_VAULT_NAME/g" 15-keyvault-secret-provider.yaml
        sed -i "s/PLACEHOLDER_TENANT_ID/$TENANT_ID/g" 15-keyvault-secret-provider.yaml  
        # sed -i "s/PLACEHOLDER_MANAGED_IDENTITY_CLIENT_ID/$AKS_ADDON_IDENTITY_CLIENT_ID/g" 15-keyvault-secret-provider.yaml
        
        # Verify replacements worked
        echo "üìã After replacement:"
        grep -E "keyvaultName|tenantId|userAssignedIdentityClientID" 15-keyvault-secret-provider.yaml
        
        # Check if any placeholders remain
        if grep -q "PLACEHOLDER" 15-keyvault-secret-provider.yaml; then
          echo "‚ùå Placeholders still exist!"
          grep "PLACEHOLDER" 15-keyvault-secret-provider.yaml
          exit 1
        fi
        
        echo "‚úÖ SecretProviderClass file updated successfully!"

    # STEP 2: NOW do the deployment with properly configured SecretProviderClass
    - name: üöÄ Blue-Green Deployment with Fixed Configuration
      run: |
        set -e
        
        echo "üöÄ Starting Blue-Green Deployment with pre-configured SecretProviderClass..."
        
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        cd kubernetes
        
        # Update image in kustomization
        echo "üìù Updating image to: ${{ env.IMAGE_WITH_DIGEST }}"
        kustomize edit set image easyshop=${{ env.IMAGE_WITH_DIGEST }}
        
        # Build manifests (SecretProviderClass already has correct values)
        echo "üîç Building manifests with configured SecretProviderClass..."
        kustomize build . > /tmp/manifests.yaml
        
        RESOURCE_COUNT=$(grep -c "^kind:" /tmp/manifests.yaml)
        echo "‚úÖ Generated $RESOURCE_COUNT resources"
        
        # Verify SecretProviderClass in manifests has real values, not placeholders
        echo "üîç Verifying SecretProviderClass in manifests..."
        if grep -q "PLACEHOLDER" /tmp/manifests.yaml; then
          echo "‚ùå CRITICAL ERROR: Placeholders found in final manifests!"
          grep -A 5 -B 5 "PLACEHOLDER" /tmp/manifests.yaml
          exit 1
        fi
        
        # Apply manifests with proper SecretProviderClass
        echo "üì¶ Applying manifests with configured SecretProviderClass..."
        kubectl apply -f /tmp/manifests.yaml
        
        # Wait for SecretProviderClass to be ready
        echo "‚è≥ Waiting for SecretProviderClass to be ready..."
        kubectl wait --for=jsonpath='{.status.phase}'=Ready secretproviderclass/easyshop-keyvault-secrets -n easyshop --timeout=120s || {
          echo "‚ö†Ô∏è SecretProviderClass not ready, but continuing..."
        }
        
        # Determine current and target deployments
        CURRENT_DEPLOYMENT=$(kubectl get svc easyshop-service -n easyshop -o jsonpath='{.spec.selector.app}' 2>/dev/null || echo "easyshop-blue")
        
        if [ "$CURRENT_DEPLOYMENT" = "easyshop-blue" ]; then
          TARGET_DEPLOYMENT="easyshop-green"
          echo "üîÑ Current: BLUE ‚Üí Target: GREEN"
        else
          TARGET_DEPLOYMENT="easyshop-blue" 
          echo "üîÑ Current: GREEN ‚Üí Target: BLUE"
        fi
        
        # Clean up existing pods from target deployment
        echo "üßπ Cleaning up existing pods from target deployment..."
        kubectl delete pods -n easyshop -l app=$TARGET_DEPLOYMENT --force --grace-period=0 || true
        
        echo "‚è≥ Waiting for cleanup to complete..."
        sleep 20
        
        # Wait for deployment rollout
        echo "‚è≥ Waiting for $TARGET_DEPLOYMENT rollout..."
        if kubectl rollout status deployment/$TARGET_DEPLOYMENT -n easyshop --timeout=600s; then
          echo "‚úÖ Rollout successful!"
          
          # Verify pods are running and secrets mounted
          echo "üîç Verifying deployment..."
          kubectl get pods -n easyshop -l app=$TARGET_DEPLOYMENT
          
          # Switch traffic
          echo "üîÄ Switching traffic to $TARGET_DEPLOYMENT..."
          kubectl patch svc easyshop-service -n easyshop \
            -p '{"spec":{"selector":{"app":"'$TARGET_DEPLOYMENT'"}}}'
          
          # Scale down old deployment
          if [ "$CURRENT_DEPLOYMENT" != "$TARGET_DEPLOYMENT" ]; then
            echo "üìâ Scaling down $CURRENT_DEPLOYMENT..."
            kubectl scale deployment/$CURRENT_DEPLOYMENT --replicas=0 -n easyshop || true
          fi
          
          echo ""
          echo "üéâ BLUE-GREEN DEPLOYMENT SUCCESSFUL!"
          echo "üéØ Active: $TARGET_DEPLOYMENT"
          echo "üêã Image: ${{ env.IMAGE_WITH_DIGEST }}"
          
        else
          echo "‚ùå Deployment rollout failed"
          echo "üìã Pod status:"
          kubectl get pods -n easyshop -l app=$TARGET_DEPLOYMENT
          echo ""
          echo "üìã Recent events:"
          kubectl get events -n easyshop --sort-by='.lastTimestamp' | tail -20
          echo ""
          echo "üìã SecretProviderClass final status:"
          kubectl describe secretproviderclass easyshop-keyvault-secrets -n easyshop
          exit 1
        fi

    - name: üîç Verify Deployment
      run: |
        echo "üîç Verifying deployment..."
        
        # Check pods
        echo "üì¶ Pods in ${{ env.NAMESPACE }}:"
        kubectl get pods -n ${{ env.NAMESPACE }}
        
        # Check services
        echo "üåê Services in ${{ env.NAMESPACE }}:"
        kubectl get svc -n ${{ env.NAMESPACE }}
        
        # Get the actual hostname from ingress
        echo "üîó Ingress in ${{ env.NAMESPACE }}:"
        kubectl get ingress -n ${{ env.NAMESPACE }}
        
        INGRESS_HOST=$(kubectl get ingress easyshop-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "${{ secrets.TF_DNS_ZONE_NAME }}")
        echo "INGRESS_HOST=$INGRESS_HOST" >> $GITHUB_ENV

    - name: üìã Deployment Summary
      run: |
        echo "üéâ Blue-Green Deployment Complete!"
        echo "=================================="
        echo "üè∑Ô∏è  SHA Version: ${{ env.VERSION }}"
        echo "üêã Image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
        echo "üìå Digest: ${{ env.IMAGE_WITH_DIGEST }}"
        echo "üéØ Cluster: ${{ env.AKS_CLUSTER_NAME }}"
        echo "üè† Namespace: ${{ env.NAMESPACE }}"
        echo "üåê URL: https://${{ env.INGRESS_HOST }}"
        echo "üìç Commit: ${{ github.sha }}"
        echo ""
        echo "üîÑ Blue-Green Strategy:"
        echo "  ‚úÖ Deployed to inactive slot with digest pinning"
        echo "  ‚úÖ Verified health and readiness probes"
        echo "  ‚úÖ Switched traffic with zero downtime"
        echo "  ‚úÖ Scaled down old deployment"
        echo "=================================="