name: EasyShop Application Deployment

on:
  workflow_dispatch:
  repository_dispatch:
    types: [deploy-applications]
  workflow_run:
    workflows: ["Terraform Infrastructure Provisioning"]
    types: [completed]
    branches: [main]

env:
  REGISTRY: ${{ secrets.TF_ACR_NAME }}.azurecr.io
  IMAGE_NAME: easyshop
  AKS_CLUSTER_NAME: ${{ secrets.TF_AKS_CLUSTER_NAME }}
  RESOURCE_GROUP: ${{ secrets.TF_PROJECT_NAME }}-rg
  NAMESPACE: easyshop

jobs:
  deploy-application:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'repository_dispatch' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') }}
    
    steps:
    - name: ğŸš€ Checkout Repository
      uses: actions/checkout@v4

    - name: ğŸ” Azure Login
      uses: azure/login@v2
      with:
        creds: |
          {
            "clientId": "${{ secrets.ARM_CLIENT_ID }}",
            "clientSecret": "${{ secrets.ARM_CLIENT_SECRET }}",
            "subscriptionId": "${{ secrets.ARM_SUBSCRIPTION_ID }}",
            "tenantId": "${{ secrets.ARM_TENANT_ID }}"
          }

    - name: ğŸ¯ Generate SHA Version
      id: version
      run: |
        VERSION="${GITHUB_SHA::8}"
        echo "VERSION=$VERSION" >> $GITHUB_ENV
        echo "ğŸ“¦ Using SHA version: $VERSION"

    - name: ğŸ‹ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: docker-container
        driver-opts: |
          image=moby/buildkit:latest
        use: true

    - name: ğŸ”‘ Login to ACR
      run: |
        az acr login --name ${{ secrets.TF_ACR_NAME }}

    - name: ğŸ—ï¸ Build and Push with Caching
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        platforms: linux/amd64
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        build-args: |
          GIT_SHA=${{ env.VERSION }}
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
        cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        outputs: type=image,name=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }},push=true

    - name: ğŸ“Œ Get Image Reference
      run: |
        IMAGE_DIGEST="${{ steps.build.outputs.digest }}"
        
        if [[ -n "$IMAGE_DIGEST" ]]; then
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${IMAGE_DIGEST}"
          echo "âœ… Got digest from build: $IMAGE_DIGEST"
        else
          IMAGE_WITH_DIGEST="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}"
          echo "âš ï¸ No digest from build, using tag: $IMAGE_WITH_DIGEST"
        fi
        
        echo "IMAGE_WITH_DIGEST=$IMAGE_WITH_DIGEST" >> $GITHUB_ENV
        echo "ğŸ“Œ Final image: $IMAGE_WITH_DIGEST"

    - name: âš™ï¸ Setup Kubernetes Tools
      run: |
        echo "ğŸ”‘ Getting AKS credentials..."
        az aks get-credentials \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --overwrite-existing
        
        echo "ğŸ“¦ Installing kustomize..."
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/

    # OPTIMIZED: Single step for Key Vault setup with proper error handling
    - name: ğŸ”§ Configure Key Vault Access and SecretProviderClass
      run: |
        set -e
        echo "ğŸ”§ Configuring Key Vault CSI Driver..."
        
        # Step 1: Get all required information
        echo "ğŸ“Š Gathering cluster and Key Vault information..."
        
        TENANT_ID=$(az account show --query tenantId -o tsv)
        KEY_VAULT_NAME=$(az keyvault list \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --query "[?starts_with(name, '${{ secrets.TF_PROJECT_NAME }}-kv')].name" \
          -o tsv | head -1)
        
        # Check if addon is already enabled (it should be from Terraform)
        ADDON_ENABLED=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query "addonProfiles.azureKeyvaultSecretsProvider.enabled" \
          -o tsv)
        
        if [[ "$ADDON_ENABLED" != "true" ]]; then
          echo "âŒ Key Vault CSI addon is not enabled! This should be enabled by Terraform."
          exit 1
        fi
        
        echo "âœ… Key Vault CSI addon is enabled"
        
        # Get the addon identity details
        ADDON_CLIENT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query "addonProfiles.azureKeyvaultSecretsProvider.identity.clientId" \
          -o tsv)
        
        ADDON_OBJECT_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query "addonProfiles.azureKeyvaultSecretsProvider.identity.objectId" \
          -o tsv)
        
        ADDON_RESOURCE_ID=$(az aks show \
          --resource-group ${{ env.RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }} \
          --query "addonProfiles.azureKeyvaultSecretsProvider.identity.resourceId" \
          -o tsv)
        
        echo "ğŸ“‹ Configuration Details:"
        echo "  Tenant ID: $TENANT_ID"
        echo "  Key Vault: $KEY_VAULT_NAME"
        echo "  Addon Client ID: $ADDON_CLIENT_ID"
        echo "  Addon Object ID: $ADDON_OBJECT_ID"
        echo "  Addon Resource ID: $ADDON_RESOURCE_ID"
        
        # Step 2: Ensure Key Vault access policy (idempotent)
        echo "ğŸ” Ensuring Key Vault access policy..."
        
        # Check if policy already exists
        EXISTING_POLICY=$(az keyvault show \
          --name "$KEY_VAULT_NAME" \
          --query "properties.accessPolicies[?objectId=='$ADDON_OBJECT_ID'].objectId" \
          -o tsv)
        
        if [[ -z "$EXISTING_POLICY" ]]; then
          echo "ğŸ“ Setting new Key Vault access policy..."
          az keyvault set-policy \
            --name "$KEY_VAULT_NAME" \
            --object-id "$ADDON_OBJECT_ID" \
            --secret-permissions get list \
            --output table
        else
          echo "âœ… Key Vault access policy already exists"
        fi
        
        # Step 3: Wait for Azure AD propagation
        echo "â³ Waiting for Azure AD propagation (30 seconds)..."
        sleep 30
        
        # Step 4: Update SecretProviderClass YAML
        echo "ğŸ“ Updating SecretProviderClass configuration..."
        cd kubernetes
        
        # Create a backup
        cp 15-keyvault-secret-provider.yaml 15-keyvault-secret-provider.yaml.bak
        
        # Replace placeholders
        sed -i "s/PLACEHOLDER_KEYVAULT_NAME/$KEY_VAULT_NAME/g" 15-keyvault-secret-provider.yaml
        sed -i "s/PLACEHOLDER_TENANT_ID/$TENANT_ID/g" 15-keyvault-secret-provider.yaml
        sed -i "s/PLACEHOLDER_CLIENT_ID/$ADDON_CLIENT_ID/g" 15-keyvault-secret-provider.yaml
        
        # Verify no placeholders remain
        if grep -q "PLACEHOLDER" 15-keyvault-secret-provider.yaml; then
          echo "âŒ Failed to replace all placeholders!"
          grep "PLACEHOLDER" 15-keyvault-secret-provider.yaml
          exit 1
        fi
        
        echo "âœ… SecretProviderClass configuration updated"
        
        # Step 5: Verify CSI driver components
        echo "ğŸ” Verifying CSI driver components..."
        
        # Check CSI driver pods
        CSI_DRIVER_READY=$(kubectl get pods -n kube-system \
          -l app=secrets-store-csi-driver \
          -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
          grep -o "True" | wc -l)
        
        CSI_PROVIDER_READY=$(kubectl get pods -n kube-system \
          -l app=secrets-store-provider-azure \
          -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
          grep -o "True" | wc -l)
        
        if [[ $CSI_DRIVER_READY -lt 2 ]] || [[ $CSI_PROVIDER_READY -lt 2 ]]; then
          echo "âš ï¸ CSI driver components not fully ready. Waiting..."
          kubectl wait --for=condition=ready pod \
            -l app=secrets-store-csi-driver \
            -n kube-system \
            --timeout=120s || true
          kubectl wait --for=condition=ready pod \
            -l app=secrets-store-provider-azure \
            -n kube-system \
            --timeout=120s || true
        else
          echo "âœ… All CSI driver components are ready"
        fi
        
        # Store values for next steps
        echo "ADDON_CLIENT_ID=$ADDON_CLIENT_ID" >> $GITHUB_ENV
        echo "KEY_VAULT_NAME=$KEY_VAULT_NAME" >> $GITHUB_ENV
        echo "TENANT_ID=$TENANT_ID" >> $GITHUB_ENV

    # OPTIMIZED: Simplified deployment with better error handling
    - name: ğŸš€ Blue-Green Deployment
      run: |
        set -e
        echo "ğŸš€ Starting Blue-Green Deployment..."
        
        # Create namespace if it doesn't exist
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        
        cd kubernetes
        
        # Update image reference
        echo "ğŸ“ Updating image to: ${{ env.IMAGE_WITH_DIGEST }}"
        kustomize edit set image easyshop=${{ env.IMAGE_WITH_DIGEST }}
        
        # Build and validate manifests
        echo "ğŸ”¨ Building Kubernetes manifests..."
        kustomize build . > /tmp/manifests.yaml
        
        # Validate manifests
        echo "âœ… Validating manifests..."
        kubectl apply --dry-run=client -f /tmp/manifests.yaml > /dev/null
        
        # Apply manifests
        echo "ğŸ“¦ Applying manifests to cluster..."
        kubectl apply -f /tmp/manifests.yaml
        
        # Determine deployment strategy
        CURRENT_DEPLOYMENT=$(kubectl get svc easyshop-service -n ${{ env.NAMESPACE }} \
          -o jsonpath='{.spec.selector.app}' 2>/dev/null || echo "none")
        
        if [[ "$CURRENT_DEPLOYMENT" == "easyshop-blue" ]]; then
          TARGET_DEPLOYMENT="easyshop-green"
          OLD_DEPLOYMENT="easyshop-blue"
        elif [[ "$CURRENT_DEPLOYMENT" == "easyshop-green" ]]; then
          TARGET_DEPLOYMENT="easyshop-blue"
          OLD_DEPLOYMENT="easyshop-green"
        else
          # First deployment
          TARGET_DEPLOYMENT="easyshop-blue"
          OLD_DEPLOYMENT="none"
        fi
        
        echo "ğŸ”„ Deployment Strategy:"
        echo "  Current: $CURRENT_DEPLOYMENT"
        echo "  Target: $TARGET_DEPLOYMENT"
        
        # Force restart target deployment to pick up new secrets
        echo "ğŸ”„ Restarting $TARGET_DEPLOYMENT deployment..."
        kubectl rollout restart deployment/$TARGET_DEPLOYMENT -n ${{ env.NAMESPACE }} || true
        
        # Wait for rollout with detailed status
        echo "â³ Waiting for $TARGET_DEPLOYMENT to be ready..."
        
        # Monitor rollout with timeout
        ROLLOUT_SUCCESS=false
        for i in {1..60}; do
          READY_REPLICAS=$(kubectl get deployment $TARGET_DEPLOYMENT -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          DESIRED_REPLICAS=$(kubectl get deployment $TARGET_DEPLOYMENT -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")
          
          echo "  Attempt $i/60: Ready replicas: $READY_REPLICAS/$DESIRED_REPLICAS"
          
          if [[ "$READY_REPLICAS" == "$DESIRED_REPLICAS" ]] && [[ "$READY_REPLICAS" != "0" ]]; then
            ROLLOUT_SUCCESS=true
            break
          fi
          
          # Check for mount errors
          MOUNT_ERRORS=$(kubectl get events -n ${{ env.NAMESPACE }} \
            --field-selector involvedObject.kind=Pod \
            --field-selector reason=FailedMount \
            -o json | jq -r '.items[-1].message' 2>/dev/null || echo "")
          
          if [[ -n "$MOUNT_ERRORS" ]] && [[ "$MOUNT_ERRORS" != "null" ]]; then
            echo "  âš ï¸ Mount error detected: $MOUNT_ERRORS"
            
            # If identity error, try to fix it
            if echo "$MOUNT_ERRORS" | grep -q "ManagedIdentityCredential"; then
              echo "  ğŸ”§ Attempting to fix identity issue..."
              
              # Delete pods to force recreation
              kubectl delete pods -n ${{ env.NAMESPACE }} \
                -l app=$TARGET_DEPLOYMENT \
                --force --grace-period=0 || true
              
              sleep 10
            fi
          fi
          
          sleep 10
        done
        
        if [[ "$ROLLOUT_SUCCESS" == "true" ]]; then
          echo "âœ… Deployment rollout successful!"
          
          # Switch traffic
          echo "ğŸ”€ Switching traffic to $TARGET_DEPLOYMENT..."
          kubectl patch svc easyshop-service -n ${{ env.NAMESPACE }} \
            -p '{"spec":{"selector":{"app":"'$TARGET_DEPLOYMENT'"}}}'
          
          # Scale down old deployment
          if [[ "$OLD_DEPLOYMENT" != "none" ]] && [[ "$OLD_DEPLOYMENT" != "$TARGET_DEPLOYMENT" ]]; then
            echo "ğŸ“‰ Scaling down $OLD_DEPLOYMENT..."
            kubectl scale deployment/$OLD_DEPLOYMENT --replicas=0 -n ${{ env.NAMESPACE }} || true
          fi
          
          echo "ğŸ‰ Blue-Green deployment completed successfully!"
        else
          echo "âŒ Deployment failed after 10 minutes"
          
          # Detailed diagnostics
          echo "ğŸ“‹ Pod Status:"
          kubectl get pods -n ${{ env.NAMESPACE }} -l app=$TARGET_DEPLOYMENT
          
          echo "ğŸ“‹ Pod Descriptions:"
          kubectl describe pods -n ${{ env.NAMESPACE }} -l app=$TARGET_DEPLOYMENT | tail -100
          
          echo "ğŸ“‹ Recent Events:"
          kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' | tail -20
          
          exit 1
        fi

    - name: ğŸ” Verify Deployment
      if: success()
      run: |
        echo "ğŸ” Final deployment verification..."
        
        # Check all components
        echo "ğŸ“¦ Pods:"
        kubectl get pods -n ${{ env.NAMESPACE }}
        
        echo "ğŸŒ Services:"
        kubectl get svc -n ${{ env.NAMESPACE }}
        
        echo "ğŸ”— Ingress:"
        kubectl get ingress -n ${{ env.NAMESPACE }}
        
        # Get application URL
        INGRESS_HOST=$(kubectl get ingress easyshop-ingress -n ${{ env.NAMESPACE }} \
          -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "${{ secrets.TF_DNS_ZONE_NAME }}")
        
        echo ""
        echo "ğŸ‰ Deployment Complete!"
        echo "=================================="
        echo "ğŸ·ï¸ Version: ${{ env.VERSION }}"
        echo "ğŸ‹ Image: ${{ env.IMAGE_WITH_DIGEST }}"
        echo "ğŸŒ URL: https://$INGRESS_HOST"
        echo "=================================="
        